{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522b3a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d2be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = \"img/PROFILE.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c7f2d",
   "metadata": {},
   "source": [
    "# RGB Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d04be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(img): <class 'numpy.ndarray'>\n",
      "shape of RGB: (4032, 3024, 3)\n",
      "dtype of RGB: uint8\n",
      "unique values in RGB: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255]\n",
      "unique values sum in RGB: 256\n",
      "size (pixels): 36578304\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(pic)\n",
    "\n",
    "cv2.namedWindow(\"output\",cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"output\",900,900)\n",
    "\n",
    "cv2.imshow(\"output\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"type(img):\", type(img))\n",
    "print(\"shape of RGB:\", img.shape)\n",
    "print(\"dtype of RGB:\", img.dtype)\n",
    "print(\"unique values in RGB:\", np.unique(img))\n",
    "print(\"unique values sum in RGB:\", len(np.unique(img)))\n",
    "print(\"size (pixels):\", img.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd993322",
   "metadata": {},
   "source": [
    "# What is Bit mean in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f348c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 bit -> can store 2 values (0,1)\n",
    "# 8 bit -> can store 256 values (0-255)\n",
    "# 16 bit -> can store 65536 values (0-65535)\n",
    "# 32 bit -> can store 4294967296 values (0-4294967295)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf11d3fa",
   "metadata": {},
   "source": [
    "# Bit depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b074e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bit Depth in Images = Number of bits used to represent the color of a single pixel.\n",
    "# why it matters? = It determines the range of colors and shades that can be represented in an image. Higher bit depth allows for more precise color representation and smoother gradients.\n",
    "# Common Bit Depths:\n",
    "# 1-bit: Black and white images (2 colors).\n",
    "# 8-bit: Grayscale images (256 shades of gray) or indexed color images (256 colors).\n",
    "# 24-bit: True color images (16.7 million colors, 8 bits per channel for RGB).\n",
    "# 48-bit: High color depth images (over 281 trillion colors, 16 bits per channel for RGB).  \n",
    "# Use Cases:\n",
    "# 1-bit: Simple graphics, icons, and binary images.\n",
    "# 8-bit: Web graphics, simple photographs, and images with limited color palettes.\n",
    "# 24-bit: Standard photographs, digital images, and most computer graphics.\n",
    "# 48-bit: Professional photography, medical imaging, and scientific visualization where color accuracy is crucial.  \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0752ae42",
   "metadata": {},
   "source": [
    "# Black and White(0,1)(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec7b49c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary shape: (4032, 3024)\n",
      "binary dtype: float64\n",
      "unique values in binary: [0. 1.]\n",
      "unique values in binary: 2\n",
      "size (pixels) in binary: 12192768\n"
     ]
    }
   ],
   "source": [
    "img_binary = cv2.imread(pic,cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "_, binary_img = cv2.threshold(img_binary,127,255,cv2.THRESH_BINARY)\n",
    "binary_img_01 = binary_img / 255\n",
    "\n",
    "cv2.namedWindow(\"output\",cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"output\",900,900)\n",
    "\n",
    "cv2.imshow(\"output\",binary_img_01*255)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"binary shape:\", binary_img_01.shape)\n",
    "print(\"binary dtype:\", binary_img_01.dtype)\n",
    "print(\"unique values in binary:\", np.unique(binary_img_01))\n",
    "print(\"unique values in binary:\", len(np.unique(binary_img_01)))\n",
    "print(\"size (pixels) in binary:\", binary_img_01.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc62ce7",
   "metadata": {},
   "source": [
    "# GrayScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d34c510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gray shape: (4032, 3024)\n",
      "Gray dtype: uint8\n",
      "unique values in Gray: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255]\n",
      "unique values in Gray: 256\n",
      "size (pixels) in Gray: 12192768\n"
     ]
    }
   ],
   "source": [
    "Gray_img = cv2.imread(pic,cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "cv2.namedWindow(\"output\",cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"output\",900,900)\n",
    "\n",
    "cv2.imshow(\"output\",Gray_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Gray shape:\", Gray_img.shape)\n",
    "print(\"Gray dtype:\", Gray_img.dtype)\n",
    "print(\"unique values in Gray:\", np.unique(Gray_img))\n",
    "print(\"unique values in Gray:\", len(np.unique(Gray_img)))\n",
    "print(\"size (pixels) in Gray:\", Gray_img.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e36a46",
   "metadata": {},
   "source": [
    "# Pixels Size Difference in this 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b9a795b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel size of RGB, Gray, binary: 36578304 12192768 12192768\n"
     ]
    }
   ],
   "source": [
    "print(\"pixel size of RGB, Gray, binary:\", img.size, Gray_img.size, binary_img_01.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7c97f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of pixels in RGB image: 12192768\n",
      "This show that each pixel in RGB image has 3 channels (R,G,B), and 3 times number of the other 2 channels).\n"
     ]
    }
   ],
   "source": [
    "num = img.size // 3\n",
    "print(\"num of pixels in RGB image:\", num)\n",
    "print(\"This show that each pixel in RGB image has 3 channels (R,G,B), and 3 times number of the other 2 channels).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1155e59",
   "metadata": {},
   "source": [
    "# Play With the pixel by changing the number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db5da355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 720, 3)\n"
     ]
    }
   ],
   "source": [
    "L = 200\n",
    "B = 720\n",
    "C =  3  # Don't put channel = 2 \n",
    "Bin = binary_img_01\n",
    "Gray = Gray_img\n",
    "RGB = img\n",
    "im = np.resize(RGB, (L, B, C))\n",
    "cv2.namedWindow(\"output\",cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"output\",900,900) \n",
    "cv2.imshow(\"output\",im)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc5aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "imB = img[:,:,0]\n",
    "imG = img[:,:,1]\n",
    "imR = img[:,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57631301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4032, 9072)\n"
     ]
    }
   ],
   "source": [
    "new_img = np.hstack((imB,imG,imR))\n",
    "cv2.namedWindow(\"output\",cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"output\",900,900)\n",
    "cv2.imshow(\"output\",new_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(new_img.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a67c97d",
   "metadata": {},
   "source": [
    "# See the different RGB color in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb36c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "im__= cv2.imread(pic)\n",
    "img_ = cv2.cvtColor(im__,cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e971ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4032, 9072, 3)\n",
      "Shape of concatenated image: (4032, 9072, 3)\n",
      "Original shape: (4032, 3024, 3)\n",
      "Individual channel shapes: (4032, 3024), (4032, 3024), (4032, 3024)\n"
     ]
    }
   ],
   "source": [
    "imB = img_[:,:,0]\n",
    "imG = img_[:,:,1]\n",
    "imR = img_[:,:,2]\n",
    "\n",
    "imR_3c = cv2.merge([imR, np.zeros_like(imR), np.zeros_like(imR)])\n",
    "imG_3c = cv2.merge([np.zeros_like(imG), imG, np.zeros_like(imG)])\n",
    "imB_3c = cv2.merge([np.zeros_like(imB), np.zeros_like(imB), imB])\n",
    "\n",
    "\n",
    "new_img = np.hstack((imB_3c,imG_3c,imR_3c))\n",
    "cv2.namedWindow(\"output\",cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"output\",1800,600)\n",
    "cv2.imshow(\"output\",new_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(new_img.shape)\n",
    "print(f\"Shape of concatenated image: {new_img.shape}\")\n",
    "print(f\"Original shape: {img_.shape}\")\n",
    "print(f\"Individual channel shapes: {imR.shape}, {imG.shape}, {imB.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb2d1e",
   "metadata": {},
   "source": [
    "# Resize the image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eea58d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_img = cv2.resize(im__, (3300,800),cv2.INTER_LANCZOS4)\n",
    "cv2.namedWindow(\"output\",cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"output\",900,900)\n",
    "cv2.imshow(\"output\",resize_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0d91a",
   "metadata": {},
   "source": [
    "# Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86051594",
   "metadata": {},
   "source": [
    "- Filters modify pixel values based on their neightbors\n",
    "- Blurring, Sharpening, Edge Detection, Embossing, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a63dc05",
   "metadata": {},
   "source": [
    "# Mean/Average Filter\n",
    "- Replace each pixel with the average of surrounding pixels.\n",
    "\n",
    "    - Effect:\n",
    "        - Smooths the image\n",
    "        - Removes noise \n",
    "        - Blurs edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6fa3be",
   "metadata": {},
   "source": [
    "# Gaussian Blur\n",
    "- Uses a Gaussian (bell-shaped) kernel to blur the image\n",
    "    - Effect:\n",
    "        - More natural blur than mean filter\n",
    "        - Best for noise removal\n",
    "        - preserves edges better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2420d985",
   "metadata": {},
   "source": [
    "# Median Blur\n",
    "- Replace each pixel with the median value of its neighbors.\n",
    "    - Best for:\n",
    "        - Removing salt & pepper noise\n",
    "        - Preserving edges clearly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8b108",
   "metadata": {},
   "source": [
    "# Bilateral Filter \n",
    "- Smooths the image while keeping edges sharp.\n",
    "    - Best for:\n",
    "        - Face smoothing \n",
    "        - Cartoon effect\n",
    "        - Edge-preserving denoising\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cee07c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "blur = cv2.blur(im__,(5,5))\n",
    "gaussian = cv2.GaussianBlur(im__,(5,5),1)\n",
    "median = cv2.medianBlur(im__,5)\n",
    "bilateral = cv2.bilateralFilter(im__,9,75,75)\n",
    "new_img = np.hstack((im__,gaussian,blur,median,bilateral))\n",
    "cv2.namedWindow(\"output\",cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"output\",900,900)\n",
    "cv2.imshow(\"output\",new_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ea710",
   "metadata": {},
   "source": [
    "# Blur the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e986324",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow(\"output\",cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"output\",900,900)\n",
    "x,y,w,h = 1100,1300,900,900 # x,y,w,h \n",
    "im_crop = im__[y:y+h, x:x+w]\n",
    "blurred_roi = cv2.blur(im_crop,(50,50))\n",
    "im__[y:y+h, x:x+w] = blurred_roi\n",
    "cv2.imshow(\"output\",im__)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790716bc",
   "metadata": {},
   "source": [
    "# Edge Detection\n",
    "- TO identify the sharp changes in brightness that typically signify object boundaries,edges,lines,or textures.\n",
    "- Edges = rapid changes in intensity\n",
    "    - Used in :\n",
    "        - Object detection,segmentation,tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f1b1de",
   "metadata": {},
   "source": [
    "# Sobel Operator\n",
    "- (edge_dection_filter)computes gradient (intensity change) in X & Y Directions\n",
    "    - Use case:\n",
    "        - For edge dection for model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc2beb",
   "metadata": {},
   "source": [
    "# Laplacian Operator\n",
    "- also a edge_dection_filter unlike sobel it is non-directional and detects edge in all directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f00ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "im__= cv2.imread(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68e324e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sobelx = cv2.Sobel(im__, cv2.CV_64F, 1, 0, ksize=7)\n",
    "sobely = cv2.Sobel(im__, cv2.CV_64F, 0, 1, ksize=7)\n",
    "laplacian = cv2.Laplacian(im__, cv2.CV_64F, ksize=13)\n",
    "sobel_combined = cv2.addWeighted(sobelx, 0.5, sobely, 0.5, 0)\n",
    "new_img = np.hstack((sobel_combined, laplacian))\n",
    "cv2.namedWindow(\"output\",cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"output\",900,900)\n",
    "cv2.imshow(\"output\",new_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b4aee",
   "metadata": {},
   "source": [
    "# Canny \n",
    "- (Edge_dection_filter)\n",
    "    - step\n",
    "        - noise reduction(gaussian)\n",
    "        - gradient calculation(sobel X,Y)\n",
    "        - Non-maximum suppression (thin edge)\n",
    "        - Hysteresis thresholding (threshold1,threshold2)\n",
    "        # code\n",
    "        - (# Example: edges = cv2.Canny(img, 50, 150)\n",
    "            - threshold1 = 50 (lower threshold)\n",
    "            - threshold2 = 150 (upper threshold))\n",
    "\n",
    "# How to choose thresholds:\n",
    "- threshold1: Lower bound (weak edges)\n",
    "\n",
    "- threshold2: Upper bound (strong edges)\n",
    "\n",
    "- Edges with gradient > threshold2: Kept\n",
    "\n",
    "- Edges between threshold1 & threshold2: Kept if connected to strong edges\n",
    "\n",
    "- Edges < threshold1: Discarded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf4dd4",
   "metadata": {},
   "source": [
    "# Why Canny is best:\n",
    "- Low error rate (few missed edges)\n",
    "\n",
    "- Good localization (edges close to true edges)\n",
    "\n",
    "- Single response (one edge per actual edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62b130a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto calculate thresholds\n",
    "def auto_canny(image, sigma=0.33):\n",
    "    v = np.median(image)\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "    return cv2.Canny(image, lower, upper)\n",
    "\n",
    "# edges = auto_canny()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "431b0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray__ = cv2.cvtColor(im__,cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray__,(5,5),1)\n",
    "edges = cv2.Canny(blur,100,200)\n",
    "cv2.namedWindow(\"output\",cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"output\",900,900)      \n",
    "cv2.imshow(\"output\",edges)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32abfaab",
   "metadata": {},
   "source": [
    "# Thresholding \n",
    "- Thresholding is a basic image processing technique that converts an image into pure black & white(binary).\n",
    "\n",
    "- It decides:\n",
    "    - Pixels above the threshold -> white(255)\n",
    "    - Pixels below the threshold -> black(0)\n",
    "\n",
    "- It is mainly used for:\n",
    "    - Removing background\n",
    "    - Preparing images for OCR\n",
    "    - Detecting shapes\n",
    "    - Scanned documents\n",
    "    - Simple segmentation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541af350",
   "metadata": {},
   "source": [
    "# Global Thresholding \n",
    "- Use one single threshold value for the entire image.\n",
    "- Example: threshold = 127\n",
    "    - Pixel > 127 ->white\n",
    "    - Pixel > 127 ->black\n",
    "- Good FOR:\n",
    "    - Images with even lighting\n",
    "    - Simple backgrounds\n",
    "    - Clear contrast\n",
    "\n",
    "- Bad for:\n",
    "    - Shadows\n",
    "    - Uneven lighting\n",
    "    - Dark ares and bright areas mixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e92149",
   "metadata": {},
   "source": [
    "# Adaptive Thresholding \n",
    "- Instead of one value, the image is divided into small region gets its own threshold.\n",
    "- Perfect when lighting is not consistent across the image.\n",
    "- Best for:\n",
    "    - Scanned documents\n",
    "    - Paper with shadows\n",
    "    - Text on uneven lighting\n",
    "    - Old,stained papers\n",
    "\n",
    "# Why it works:\n",
    "- It calculates threshold = mean value of neighborhood pixels-C\n",
    "- so local light changes don't cause problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d11106",
   "metadata": {},
   "source": [
    "# Otsu's Thresholding \n",
    "- Otsu's Method automatically finds the best global threshold based on histogram\n",
    "- You don't decide the number ; Otsu does it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d682a6",
   "metadata": {},
   "source": [
    "# When to use:\n",
    "- when you don't know the correct threshold\n",
    "- Image has two clear peaks in histogram (object + background)\n",
    "\n",
    "# How it works(simple)\n",
    "- Otsu analyzes the grayscale histogram and finds the point where:\n",
    "    - The difference between object pixels and background pixels is maximum(largest variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99dd84a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(im__,cv2.COLOR_BGR2GRAY)\n",
    "__, th = cv2.threshold(gray,127,255,cv2.THRESH_BINARY)\n",
    "adaptive_th = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "__,otsu_th = cv2.threshold(gray,0,255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "new_img = np.hstack((th, adaptive_th,otsu_th))\n",
    "cv2.namedWindow(\"output\",cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"output\",900,900)\n",
    "cv2.imshow(\"output\",new_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
